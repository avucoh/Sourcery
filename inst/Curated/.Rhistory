M2 [label = 'Populate properties with DB sources and API']
M3 [label = 'Check and fill in properties manually']
E1 [label = 'Export JSON']
# 'edge' statements
S1->M1 M1->M2 M2->M3 M3->E1
}
")
DiagrammeR::grViz("
digraph Dataset_workflow {
graph [overlap = true, fontsize = 10, rankdir = LR]
# 'node' statements
node [shape = box,
fontname = Helvetica]
S1 [label = 'Methods text', shape = oval]
M3 [label = 'Check and fill in properties manually']
E1 [label = 'Export JSON']
# 'edge' statements
{S1}->M3 M3->E1
}
")
DiagrammeR::grViz("
digraph Technology_workflow {
graph [overlap = true, fontsize = 10, rankdir = LR]
# 'node' statements
node [shape = box,
fontname = Helvetica]
S1 [label = 'Abstract/Discussion/full text', shape = oval]
M3 [label = 'Check and fill in properties manually']
M4 [label = 'Register technology if appropriate']
M5 [label = 'Update record with RRID']
E1 [label = 'Export JSON']
# 'edge' statements
{S1}->M3 M3->M4 M4->M5 M5->E1
}
")
shiny::runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
sample(c("Yes", "No"), size = 94, replace = T)
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
install.packages("sourcetools")
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
runApp('HIRN/concept')
setwd("~/HIRN/Sourcery")
abCrunch <- function(query = NULL, maxhits = 1) {
if(!length(query)) {
return(data.frame(RRID = ""))
} else {
# nif-0000-07730-1 is the SciCrunch Source id for Antibody Registry
search <- GET("https://dknet.org/api/1/dataservices/federation/data/nif-0000-07730-1?",
query=list(q = query, key = mykey),
config = httr::config(ssl_verifypeer = FALSE)) %>% content(encoding = "UTF-8")
abdata <- xml_find_first(search, "result/results") %>% xml_find_all("row")
if(length(abdata) == 0) return (NULL)
abdata <- sapply(abdata, function(x) xml_find_all(x, "data/value") %>% xml_text())
%>% gsub(pat = "<[^>]+>", rep = "")
%>% trimws()
%>% t()
abdata <- as.data.frame(abdata, stringsAsFactors = F)
abdata <- abdata[, -12] # last column is SciCrunch uuid
n <- min(nrow(abdata), maxhits)
abdata <- abdata[1:n, ]
# DKnet antibody table data headers
names(abdata) <- c("RRID", "Name", "TargetAntigen", "RRIDCite", "Clonality", "Reference", "Comments", "CloneName", "RaisedIn", "VendorName", "VendorCat")
abdata$VendorName <- gsub(" Go To Vendor", "", abdata$VendorName)
abdata$TargetAntigen <- gsub("See NCBI gene ", "", abdata$TargetAntigen)
return(abdata)
}
}
abCrunch <- function(query = NULL, maxhits = 1) {
if(!length(query)) {
return(data.frame(RRID = ""))
} else {
# nif-0000-07730-1 is the SciCrunch Source id for Antibody Registry
search <- GET("https://dknet.org/api/1/dataservices/federation/data/nif-0000-07730-1?",
query=list(q = query, key = mykey),
config = httr::config(ssl_verifypeer = FALSE)) %>% content(encoding = "UTF-8")
abdata <- xml_find_first(search, "result/results") %>% xml_find_all("row")
if(length(abdata) == 0) return (NULL)
abdata <- sapply(abdata, function(x) xml_find_all(x, "data/value") %>% xml_text())
%>% gsub(pat = "<[^>]+>", rep = "")
%>% trimws()
%>% t()
abdata <- as.data.frame(abdata, stringsAsFactors = F)
abdata <- abdata[, -12] # last column is SciCrunch uuid
n <- min(nrow(abdata), maxhits)
abdata <- abdata[1:n, ]
# DKnet antibody table data headers
names(abdata) <- c("RRID", "Name", "TargetAntigen", "RRIDCite", "Clonality", "Reference", "Comments", "CloneName", "RaisedIn", "VendorName", "VendorCat")
abdata$VendorName <- gsub(" Go To Vendor", "", abdata$VendorName)
abdata$TargetAntigen <- gsub("See NCBI gene ", "", abdata$TargetAntigen)
return(abdata)
}
}
abCrunch <- function(query = NULL, maxhits = 1) {
if(!length(query)) {
return(data.frame(RRID = ""))
} else {
# nif-0000-07730-1 is the SciCrunch Source id for Antibody Registry
search <- GET("https://dknet.org/api/1/dataservices/federation/data/nif-0000-07730-1?",
query=list(q = query, key = mykey),
config = httr::config(ssl_verifypeer = FALSE)) %>% content(encoding = "UTF-8")
abdata <- xml_find_first(search, "result/results") %>% xml_find_all("row")
if(length(abdata) == 0) return (NULL)
abdata <- sapply(abdata, function(x) xml_find_all(x, "data/value") %>% xml_text()) %>%
gsub(pat = "<[^>]+>", rep = "") %>% trimws() %>% t()
abdata <- as.data.frame(abdata, stringsAsFactors = F)
abdata <- abdata[, -12] # last column is SciCrunch uuid
n <- min(nrow(abdata), maxhits)
abdata <- abdata[1:n, ]
# DKnet antibody table data headers
names(abdata) <- c("RRID", "Name", "TargetAntigen", "RRIDCite", "Clonality", "Reference", "Comments", "CloneName", "RaisedIn", "VendorName", "VendorCat")
abdata$VendorName <- gsub(" Go To Vendor", "", abdata$VendorName)
abdata$TargetAntigen <- gsub("See NCBI gene ", "", abdata$TargetAntigen)
return(abdata)
}
}
abCrunch <- function(query = NULL, maxhits = 1) {
if(!length(query)) {
return(data.frame(RRID = ""))
} else {
# nif-0000-07730-1 is the SciCrunch Source id for Antibody Registry
search <- GET("https://dknet.org/api/1/dataservices/federation/data/nif-0000-07730-1?",
query=list(q = query, key = mykey),
config = httr::config(ssl_verifypeer = FALSE)) %>% content(encoding = "UTF-8")
abdata <- xml_find_first(search, "result/results") %>% xml_find_all("row")
if(length(abdata) == 0) return (NULL)
abdata <- sapply(abdata, function(x) xml_find_all(x, "data/value") %>% xml_text()) %>%
gsub(pat = "<[^>]+>", rep = "") %>%
trimws() %>%
t()
abdata <- as.data.frame(abdata, stringsAsFactors = F)
abdata <- abdata[, -12] # last column is SciCrunch uuid
n <- min(nrow(abdata), maxhits)
abdata <- abdata[1:n, ]
# DKnet antibody table data headers
names(abdata) <- c("RRID", "Name", "TargetAntigen", "RRIDCite", "Clonality", "Reference", "Comments", "CloneName", "RaisedIn", "VendorName", "VendorCat")
abdata$VendorName <- gsub(" Go To Vendor", "", abdata$VendorName)
abdata$TargetAntigen <- gsub("See NCBI gene ", "", abdata$TargetAntigen)
return(abdata)
}
}
abCrunch("#A2066")
library(magrittr)
abCrunch("#A2066")
devtools::load_all()
abCrunch("#A2066")
abCrunch <- function(query = NULL, maxhits = 1) {
if(!length(query)) {
return(data.frame(RRID = ""))
} else {
# nif-0000-07730-1 is the SciCrunch Source id for Antibody Registry
search <- GET("https://dknet.org/api/1/dataservices/federation/data/nif-0000-07730-1?",
query=list(q = query, key = mykey),
config = httr::config(ssl_verifypeer = FALSE)) %>% content(encoding = "UTF-8")
abdata <- xml_find_first(search, "result/results") %>% xml_find_all("row")
if(length(abdata) == 0) return (NULL)
abdata <- sapply(abdata, function(x) xml_find_all(x, "data/value") %>% xml_text()) %>%
gsub(pat = "<[^>]+>", rep = "") %>%
trimws() %>%
t()
abdata <- as.data.frame(abdata, stringsAsFactors = F)
# abdata <- abdata[, -12] # last column is SciCrunch uuid
n <- min(nrow(abdata), maxhits)
abdata <- abdata[1:n, ]
# DKnet antibody table data headers
# names(abdata) <- c("RRID", "Name", "TargetAntigen", "RRIDCite", "Clonality", "Reference", "Comments", "CloneName", "RaisedIn", "VendorName", "VendorCat")
# abdata$VendorName <- gsub(" Go To Vendor", "", abdata$VendorName)
# abdata$TargetAntigen <- gsub("See NCBI gene ", "", abdata$TargetAntigen)
return(abdata)
}
}
abCrunch("#A2066")
test <- data.table(a = 1:5, b = 1:5)
test
test[, Text := NULL]
fillCols()
test
test[, new := list(c(1:2), c(2:3), "a", "b", c(4:5))]
test
test$new
devtools::load_all()
modAntibodyApp()
nrow(NULL)
devtools::load_all()
modAntibodyApp()
install.packages("pdftools")
library(pdftools)
supp <- pdf_text("home/avu/Downloads/DB181368SupplementaryData.pdf")
supp <- pdf_text("/home/avu/Downloads/DB181368SupplementaryData.pdf")
head(supp)
str(supp)
supp[10]
supp
modAntibodyApp()
devtools::load_all()
modAntibodyApp()
regmatches("(RRID:ab_2142242 1/1000)", regexpr("1(:|/)[0-9,]{1,6}", "(RRID:ab_2142242 1/1000)"))
devtools::load_all()
modAntibodyApp()
devtools::load_all()
modAntibodyApp()
test[1, ]
test[1, ]$new
test[1, ][[new]]
test[1, new]
test[1, ]$new[[1]]
test[, new2 := c(c(1:20), c(1:5)]
test[, new2 := c(c(1:20), c(1:5), 1, 3, 5)]
test[, new2 := list(c(1:20), c(1:5), 1, 3, 5)]
test
test$new3[[1]]
devtools::load_all()
modAntibodyApp()
devtools::load_all()
modAntibodyApp()
devtools::load_all()
modAntibodyApp()
jsonlite::fromJSON("http://test-resourcebrowser.azurewebsites.net/api/v1.0/AdminResource/GetNew/Antibody")
setwd("~/HIRN")
jsonlite::fromJSON("http://dev-resourcebrowser2.azurewebsites.net/api/v1.0/AdminResource/GetNew/CellLine")
jsonlite::fromJSON("http://test-resourcebrowser.azurewebsites.net/api/v1.0/AdminResource/GetNew/CellLine")
test <- try(jsonlite::fromJSON(paste0("http://test-resourcebrowser.azurewebsites.net/api/v1.0/AdminResource/GetNew/", "test")))
test
test <- try(jsonlite::fromJSON(paste0("http://test-resourcebrowser.azurewebsites.net/api/v1.0/AdminResource/GetNew/", "x")))
test
test <- try(jsonlite::fromJSON(paste0("http://test-resourcebrowser.azurewebsites.net/api/v1.0/AdminResource/GetNew/")))
class(test)
test <- readLines("publications_list.txt")
head(teast)
head(test)
pubs <- readLines("publications_list.txt")
pub_date <- seq.int(1, by = 7, length.out = length(pubs))
tail(pub_date)
pub_date <- seq.int(1, to = length(pubs), by = 7)
head(pub_date)
pub_date <- seq.int(1, to = length(pubs), by = 6)
pub_title <- seq.int(2, to = length(pubs), by = 6)
pub_auth <- seq.int(4, to = length(pubs), by = 6)
pub_source <- seq.int(5, to = length(pubs), by = 6)
pub_date <- pubs[seq.int(1, to = length(pubs), by = 6)]
pub_title <- pubs[seq.int(2, to = length(pubs), by = 6)]
pub_auth <- pubs[seq.int(4, to = length(pubs), by = 6)]
pub_source <- pubs[seq.int(5, to = length(pubs), by = 6)]
head(pub_date)
tail(pub_date)
pub_date
pubs <- readLines("publications_list.txt")
pub_date <- pubs[seq.int(1, to = length(pubs), by = 6)]
pub_title <- pubs[seq.int(2, to = length(pubs), by = 6)]
pub_auth <- pubs[seq.int(4, to = length(pubs), by = 6)]
pub_source <- pubs[seq.int(5, to = length(pubs), by = 6)]
pub_date
pubs <- readLines("publications_list.txt")
pub_date <- pubs[seq.int(1, to = length(pubs), by = 6)]
pub_title <- pubs[seq.int(2, to = length(pubs), by = 6)]
pub_auth <- pubs[seq.int(4, to = length(pubs), by = 6)]
pub_source <- pubs[seq.int(5, to = length(pubs), by = 6)]
pub_date
pub_title
pubs <- readLines("publications_list.txt")
pub_date <- pubs[seq.int(1, to = length(pubs), by = 6)]
pub_title <- pubs[seq.int(2, to = length(pubs), by = 6)]
pub_auth <- pubs[seq.int(4, to = length(pubs), by = 6)]
pub_source <- pubs[seq.int(5, to = length(pubs), by = 6)]
pub_date
pubs <- readLines("publications_list.txt")
pub_date <- pubs[seq.int(1, to = length(pubs), by = 6)]
pub_title <- pubs[seq.int(2, to = length(pubs), by = 6)]
pub_auth <- pubs[seq.int(4, to = length(pubs), by = 6)]
pub_source <- pubs[seq.int(5, to = length(pubs), by = 6)]
pub_date
pubs <- readLines("publications_list.txt")
pub_date <- pubs[seq.int(1, to = length(pubs), by = 6)]
pub_date
pub_title <- pubs[seq.int(2, to = length(pubs), by = 6)]
pub_title
pub_auth <- pubs[seq.int(4, to = length(pubs), by = 6)]
pub_auth
pub_source <- pubs[seq.int(5, to = length(pubs), by = 6)]
pub_source
pubsdt <- data.table(Date = pub_date, Title = pub_title, Authors = pub_auth, Source = pub_source)
head(pubsdt)
A <- fread("pubs2.csv")
head(A)
pmids <- regmatches(pub_source, gregexpr("(?<=PMID: ?)[0-9]+", pub_source, perl = T))
pmids <- regmatches(pub_source, gregexpr("(?<=PMID:) ?[0-9]+", pub_source, perl = T))
pmids
pmids <- sapply(pmids, function(x) if(length(x)) trimws(x) else "")
pmids
pubsdt <- data.table(Date = pub_date, Title = pub_title, Authors = pub_auth, Source = pub_source, PMID = pmids)
A <- fread("pubs2.csv")
head(A)
setnames(A, "ReviewArticle", "ArticleType")
A <- fread("pubs2.csv")
A[, ReviewArticle := ifelse(ReviewArticle == 0, "research", "review")]
A[, ReviewArticle := ifelse(CaseReport == 0, "research", "case report")]
setnames(A, "ReviewArticle", "ArticleType")
A
A$ArticleType
A[, LegacyCycle := 1]
A[, LegacyCycle := NULL]
A[, Release := 1]
head(A)
pubsdt <- merge(pubsdt, A[, .(PMID, PMC, ArticleType, Release)], by = "PMID")
class(pubsdt$PMID)
pubsdt <- data.table(Date = pub_date, Title = pub_title, Authors = pub_auth, Source = pub_source, PMID = as.numeric(pmids))
pubsdt <- merge(pubsdt, A[, .(PMID, PMC, ArticleType, Release)], by = "PMID")
head(pubsdt)
B <- pubsdt[is.na(Release), ]
pubsdt$Release
pubsdt <- merge(pubsdt, A[, .(PMID, PMC, ArticleType, Release)], by = "PMID", all = T)
pubsdt <- data.table(Date = pub_date, Title = pub_title, Authors = pub_auth, Source = pub_source, PMID = as.numeric(pmids))
pubsdt <- merge(pubsdt, A[, .(PMID, PMC, ArticleType, Release)], by = "PMID", all = T)
B <- pubsdt[is.na(Release), ]
A <- pubsdt[Release == 1]
A
write.csv(A, "publications_I.csv")
pub_date <- as.Date(pub_date, "%m/%d/%Y")
pub_date
pub_tags <- pubs[seq.int(1, to = length(pubs), by = 6)]
pub_date <- as.Date(pub_tags, "%m/%d/%Y")
class(pub)date
class(pub_date)
pub_date <- as.character(as.Date(pub_tags, "%m/%d/%Y"))
pub_consort <- sapply(pub_tags, function(x) substr(x, 11, length(x)))
pub_consort
pub_consort <- sapply(pub_tags, function(x) substr(x, 11, nchar(x)))
pub_consort
pubsdt <- data.table(Date = pub_date,
Consortium = pub_consort,
Title = pub_title,
Authors = pub_auth,
Source = pub_source,
PMID = as.numeric(pmids))
pubsdt[, is.na(PMID)]
pubsdt[, is.na(PMID), ]
pubsdt[is.na(PMID), ]
A <- fread("pubs2.csv")
A[, ArticleType := if(ReviewArticle == 1) "review" else if (CaseReport == 1) "case report" else "research"]
A
A[, ArticleType := unlist(Map(function(review, case) if(review) "review" else if(case) "case report" else "research"), ReviewArticle, CaseReport)]
Map(function(review, case) if(review) "review" else if(case) "case report" else "research"), A$ReviewArticle, A$CaseReport)
Map(function(review, case) if(review) "review" else if(case) "case report" else "research", A$ReviewArticle, A$CaseReport)
A[, ArticleType := unlist(Map(function(review, case) if(review) "review" else if(case) "case report" else "research", ReviewArticle, CaseReport)]
A[, ArticleType := unlist(Map(function(review, case) if(review) "review" else if(case) "case report" else "research", ReviewArticle, CaseReport))]
A
A[, Release := 1]
A <- merge(A[, .(PMID, PMC, ArticleType, Release)], pubsdt, all.x = T, all.y = F, by = "PMID")
A
A <- fread("pubs2.csv")
A[, ArticleType := unlist(Map(function(review, case) if(review) "review" else if(case) "case report" else "research", ReviewArticle, CaseReport))]
A[, Release := 1]
A$PMID
pubsdt$PMID
NA %in% NA
B <- pubsdt[PMID %in% A$PMID[!is.na(A$PMID)]]
B <- pubsdt[!PMID %in% A$PMID[!is.na(A$PMID)]]
B
pub_date <- as.Date(pub_tags, "%m/%d/%Y")
pubsdt <- data.table(Date = pub_date,
Consortium = pub_consort,
Title = pub_title,
Authors = pub_auth,
Source = pub_source,
PMID = as.numeric(pmids))
A <- fread("pubs2.csv")
A[, ArticleType := unlist(Map(function(review, case) if(review) "review" else if(case) "case report" else "research", ReviewArticle, CaseReport))]
A[, Release := 1]
A <- merge(A[, .(PMID, PMC, ArticleType, Release)], pubsdt[!is.na(PMID)], all.x = T, all.y = F, by = "PMID")
max(A$Date)
A$Date
pubsdt$Date
max(A$Date, na.rm = T)
min(A$Date, na.rm = T)
B <- pubsdt[Date > max(A$Date, na.rm = T)]
154 + 172
B <- pubsdt[Date >= max(A$Date, na.rm = T)]
B <- pubsdt[Date > max(A$Date, na.rm = T)]
table(is.duplicated(pmids))
table(duplicated(pmids))
pubsdt[, duplicated(pmids)]
pubsdt[duplicated(pmids), ]
pub_date
write.csv(pubsdt, "pubs_reviewed_2019-08-22.csv", row.names = F)
pubs <- fread("pubs_reviewed_2019-08-22.csv")
class(pubs$Date)
pubs[, Date := as.Date(Date)]
A <- fread("pubs2.csv")
A[, ArticleType := unlist(Map(function(review, case) if(review) "review" else if(case) "case report" else "research", ReviewArticle, CaseReport))]
A[, Release := 1]
A <- merge(A[, .(PMID, PMC, ArticleType, Release)], pubsdt[!is.na(PMID)], all.x = T, all.y = F, by = "PMID")
A
A$PMID
max(A$Date, na.rm = T)
B <- pubsdt[Date > max(A$Date, na.rm = T)]
B <- pubs[Date > max(A$Date, na.rm = T)]
pubs$Date
min(A$Date, na.rm = T)
pubs[Date < min(A$Date, na.rm = T)]
A$Date
A <- fread("pubs2.csv")
A[, ArticleType := unlist(Map(function(review, case) if(review) "review" else if(case) "case report" else "research", ReviewArticle, CaseReport))]
A[, Release := 1]
A <- merge(A[, .(PMID, PMC, ArticleType, Release)], pubs[!is.na(PMID)], all.x = T, all.y = F, by = "PMID")
A$Date
pubs$Date
A <- fread("pubs2.csv")
A[, ArticleType := unlist(Map(function(review, case) if(review) "review" else if(case) "case report" else "research", ReviewArticle, CaseReport))]
A[, Release := 1]
A$PMID
A_temp <- merge(A[, .(PMID, PMC, ArticleType, Release)], pubs[!is.na(PMID)], all.x = T, all.y = F, by = "PMID")
A_temp[is.na(Date)]
missing <- na.omit(A_temp[is.na(Date)]$PMID)
missing
pubs[PMID %in% missing]
class(missing)
class(pubs$PMID)
A[PMID %in% missing]
A
A <- merge(A[, .(PMID, PMC, ArticleType, Release)], pubs[!is.na(PMID)], all.x = T, all.y = F, by = "PMID")
A
write.csv(A, "pubs_I.csv", row.names = F)
A
B <- pubs[Date > max(A$Date, na.rm = T)]
pubs$Date
rm(A_temp)
rm(pubs)
rm(pubsdt)
pubs <- fread("pubs_reviewed_2019-08-22.csv")
pubs[, Date := as.Date(Date)]
C <- pubs[(PMID %in% A$PMID && PMID %in% B$PMID)]
pubs$PMID
C <- pubs[Date < min(A$Date, na.rm = T)]
154+172
C <- pubs[!(PMID %in% A$PMID)]
154+220
C <- pubs[!(PMID %in% na.omit(A$PMID))]
table(duplicated(pubs$PMID))
head(C)
result <- RISmed::EUtilsGet(as.character(C$PMID))
artype <- sapply(result, PublicationType)
artype <- sapply(result, RISmed::PublicationType)
head(result)
artype <- PublicationType(result)
artype <- RISmed::PublicationType(result)
artype
C$PMID
unique(unlist(artype))
research <- sapply(artype, function(x) "Journal Article" %in% metadata)
research <- sapply(artype, function(x) "Journal Article" %in% x)
artype[!research]
classifyArticle <- function(metadata) if("Journal Article" %in% metadata) { if("Review" %in% metadata) "review" else "research") } else "non-journal"
classifyArticle <- function(metadata) if("Journal Article" %in% metadata) { if("Review" %in% metadata) "review" else "research" } else "non-journal"
artype <- sapply(artype, classifyArticle)
artype
table(artype)
C[, c("PMC", "ArticleType") := NA]
C[!is.na(PMID), ArticleType := artype]
C$ArticleType
C[, ArticleType := NA_character_]
C[, ArticleType := NULL]
C[, PMC := NULL]
C[!is.na(PMID), ArticleType := artype]
C
write.csv(C, "pubs_II.csv", row.names = F)
A <- fread("pubs2.csv")
A[, ArticleType := unlist(Map(function(review, case) if(review) "review" else if(case) "case report" else "research", ReviewArticle, CaseReport))]
A[, Release := 1]
A <- merge(A[!is.na(PMID), .(PMID, PMC, ArticleType, Release)], pubs[!is.na(PMID)], all.x = T, all.y = F, by = "PMID")
write.csv(A, "pubs_I.csv", row.names = F)
C[, Release := 2]
names(C)
setcolorder(C, c("PMID"), names(C)[names(C) != "PMID"])
setcolorder(C, c("PMID", names(C)[names(C) != "PMID"]))
write.csv(C, "pubs_II.csv", row.names = F)
setwd("~/HIRN/Sourcery/inst/Curated")
ds <- fread("Ds.txt")
head(ds)
ds_report <- C[, .(PMID)]
ds_report[, Status := sapply(Status, function(x) if(x %in% ds$DefiningManuscriptId) "harvested" else "to do")]
ds_report[, Status := sapply(PMID, function(x) if(x %in% ds$DefiningManuscriptId) "harvested" else "to do")]
ds_report
C$PMID
write.csv(ds_report, "ds_report.csv")
write.csv(ds_report, "ds_report.csv", row.names = F)
"30936569" %in% ds$DefiningManuscriptId
30936569 %in% ds$DefiningManuscriptId
head(ds)
"GSE117454" %in% ds$ACC
30760760 %in% ds$DefiningManuscriptId
"30760760" %in% ds$DefiningManuscriptId
"GSE120299" %in% ds$DefiningManuscriptId
nrow(ds)
"GSE106148" %in% ds$DefiningManuscriptId
"GSE106148" %in% ds$DefiningManuscriptId
